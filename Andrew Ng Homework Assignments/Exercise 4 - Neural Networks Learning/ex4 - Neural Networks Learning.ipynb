{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X'] #5000x400.\n",
    "intNumSamples = X.shape[0]\n",
    "'''\n",
    "Add column for bias term.\n",
    "'''\n",
    "inputSize = X.shape[1] #400\n",
    "\n",
    "X = np.insert(X,0,np.ones(intNumSamples),axis =1) #5000x401.\n",
    "y = data['y'] #5000x1.\n",
    "\n",
    "hiddenSize = 25\n",
    "outputSize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 199.5, 19.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ce052904e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexRandRows = np.random.choice(X.shape[0],10)\n",
    "'''\n",
    "reshape(-1,20) rolls each 1x400 row back into a 20x20 grid.\n",
    "we can plot these as an image since each grid is a 20x20 grid of grayscale pixel values.\n",
    "'''\n",
    "plt.imshow(X[indexRandRows,1:].reshape(-1,20).T, cmap = 'gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputLayerSize,hiddenLayerSize,outputLayerSize):\n",
    "        '''\n",
    "        Layer sizes for three layer nn. Include bias terms.\n",
    "        '''\n",
    "        self.inputLayerSize = inputLayerSize #401.\n",
    "        self.hiddenLayerSize = hiddenLayerSize #26.\n",
    "        self.outputLayerSize = outputLayerSize #10.\n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def sigmoidGrad(self,z):\n",
    "        sig = self.sigmoid(z)\n",
    "        return np.multiply(sig,(1 - sig))\n",
    "    def unrollThetas(self, allParameters):\n",
    "        theta1Length = self.inputLayerSize*(self.hiddenLayerSize-1)\n",
    "        initialTheta = np.matrix(allParameters)\n",
    "        '''\n",
    "        Need to separate the thetas so that we can forwardPropagate,getCost, and backPropagate.\n",
    "        '''\n",
    "        self.theta1 = np.matrix(np.reshape(initialTheta[:,:theta1Length],(self.hiddenLayerSize-1,self.inputLayerSize))) #25x401.\n",
    "        self.theta2 = np.matrix(np.reshape(initialTheta[:,theta1Length:],(self.outputLayerSize,self.hiddenLayerSize))) #10x26.\n",
    "    def rollThetas(self,theta1,theta2):\n",
    "        '''\n",
    "        Need to roll all parameters into one array because fmin only has one initialTheta parameter.\n",
    "        '''\n",
    "        self.allParameters = np.concatenate((theta1.ravel(),theta2.ravel()))\n",
    "        return self.allParameters\n",
    "    def forwardPropagate(self,X,theta1,theta2):\n",
    "        numSamples = X.shape[0]\n",
    "        '''\n",
    "        Input layer.\n",
    "        '''\n",
    "        self.a1 = X\n",
    "        self.z2 = np.dot(self.a1,theta1.T)\n",
    "        '''\n",
    "        Hidden layer.\n",
    "        '''\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.a2 = np.insert(self.a2,0,np.ones(numSamples),axis =1)\n",
    "        '''\n",
    "        Output layer.\n",
    "        '''\n",
    "        self.z3 = np.dot(self.a2,theta2.T)\n",
    "        self.hypothesis = np.matrix(self.sigmoid(self.z3))\n",
    "        return self.hypothesis\n",
    "    def getCost(self,allParameters,X,y,regFactor):\n",
    "        '''\n",
    "        unroll allParameters into Theta1 and Theta2.\n",
    "        '''\n",
    "        self.unrollThetas(allParameters)\n",
    "        \n",
    "        self.forwardPropagate(X,self.theta1,self.theta2) #hypothesis is mxk.\n",
    "        numSamples,numLabels = self.hypothesis.shape \n",
    "        J = 0\n",
    "        for i in range(1,numLabels+1):\n",
    "            tempY = np.matrix([1 if label==i else 0 for label in y]).T #mx1\n",
    "            tempA3 = self.hypothesis[:,i-1]\n",
    "            J -= (np.dot(tempY.T,np.log(tempA3)) + np.dot((1-tempY.T),np.log(1-tempA3)))\n",
    "        J = J/numSamples\n",
    "        '''\n",
    "        Regularized portion of cost function.\n",
    "        '''\n",
    "        R = (np.sum(np.sum(np.square(self.theta1[:,1:])))+np.sum(np.sum(np.square(self.theta2[:,1:]))))*regFactor/2/numSamples\n",
    "        self.J = J+R\n",
    "        return self.J\n",
    "    def backPropagate(self,allParameters,X,y,regFactor):\n",
    "        '''\n",
    "        unroll allParameters into Theta1 and Theta2.\n",
    "        '''\n",
    "        self.unrollThetas(allParameters)\n",
    "        '''\n",
    "        propagate forward so that you can calculate the activation functions.\n",
    "        '''\n",
    "        self.forwardPropagate(X,self.theta1,self.theta2) #hypothesis is 5000x10.\n",
    "        numSamples, numLabels = self.hypothesis.shape\n",
    "        hiddenLayerSize = self.z2.shape[1]\n",
    "        Z2 = np.insert(self.z2,0,np.ones(numSamples),axis =1)\n",
    "        \n",
    "        Delta1 = np.zeros(theta1.shape) #25x401.\n",
    "        Delta2 = np.zeros(theta2.shape) #10x26.\n",
    "        \n",
    "        yMatrix = np.matrix(pd.get_dummies(y.ravel())) #5000x10.\n",
    "        for t in range(0,numSamples):\n",
    "            d3 = (self.hypothesis[t,:] - yMatrix[t,:]).T #10x1\n",
    "            '''\n",
    "            theta2 = 10x26. => theta2.Txd3 = 26x1. Z2 = 5000x26 (with bias column).\n",
    "            element wise multiplication of (theta2.Txd3)x(z2[t,:].T) results in a 26x1 vector.\n",
    "            '''\n",
    "            d2 = np.matrix(np.multiply(np.dot(self.theta2.T,d3),self.sigmoidGrad(Z2[t,:]).T)) #26x1.\n",
    "            d2 = d2[1:,:] #25x1.\n",
    "            a1Matrix = np.matrix(self.a1[t,:])\n",
    "            a2Matrix = np.matrix(self.a2[t,:])\n",
    "            Delta1 += np.dot(d2,a1Matrix) #a1Matrix = 1x401 => Delta1 = 25x401.\n",
    "            Delta2 += np.dot(d3,a2Matrix) #a2Matrix = 1x26. => Delta2 = 10x26.\n",
    "        Delta1 = Delta1/numSamples\n",
    "        Delta2 = Delta2/numSamples\n",
    "        '''\n",
    "        Regularization\n",
    "        '''\n",
    "        Delta1[:,1:] = Delta1[:,1:] + self.theta1[:,1:]*regFactor/numSamples\n",
    "        Delta2[:,1:] = Delta2[:,1:] + self.theta2[:,1:]*regFactor/numSamples\n",
    "        \n",
    "        self.grad = np.concatenate((Delta1.ravel(),Delta2.ravel()))\n",
    "        return self.grad\n",
    "    def numericalGradient(self,allParameters,X,y,regFactor,numComputations):\n",
    "        '''\n",
    "        Calculate the slope of the cost function and use it to check that backPropagate() was implemented correctly.\n",
    "        '''\n",
    "        self.gradApprox = np.zeros(numComputations)\n",
    "        numSamples = allParameters.shape[0]\n",
    "        e = 10**-4\n",
    "        \n",
    "        randIndexes = np.random.choice(numSamples,numComputations)\n",
    "        for i in range(0,numComputations):\n",
    "            thetaPlus = np.matrix(allParameters)\n",
    "            thetaMinus = np.matrix(allParameters)\n",
    "            thetaPlus[0,randIndexes[i]] = allParameters[randIndexes[i]] + e\n",
    "            thetaMinus[0,randIndexes[i]] = allParameters[randIndexes[i]] - e\n",
    "            self.gradApprox[i] = (self.getCost(thetaPlus,X,y,regFactor) - self.getCost(thetaMinus,X,y,regFactor))/2/e       \n",
    "        return self.gradApprox,randIndexes\n",
    "    def checkNNGradient(self,allParameters,X,y,regFactor,numComparisons):\n",
    "        numGrad, randIndexes = self.numericalGradient(allParameters,X,y,regFactor,numComparisons)\n",
    "        bpGrad = self.backPropagate(allParameters,X,y,regFactor)\n",
    "        bpGrad = bpGrad[randIndexes]\n",
    "        \n",
    "        diff = np.linalg.norm(numGrad - bpGrad)/np.linalg.norm(numGrad+bpGrad)\n",
    "        return diff,numGrad,bpGrad[:numComparisons],randIndexes\n",
    "    def randInitializeWeights(self, initialEpsilon = 0.12):\n",
    "        '''\n",
    "        weights should be a random value in [-initialEpsilon,initialEpsilon].\n",
    "        '''\n",
    "        self.theta1 = np.random.rand(self.hiddenLayerSize -1, self.inputLayerSize)*2*initialEpsilon - initialEpsilon\n",
    "        self.theta2 = np.random.rand(self.outputLayerSize,self.hiddenLayerSize)*2*initialEpsilon - initialEpsilon\n",
    "        self.rollThetas(self.theta1,self.theta2)\n",
    "        return self.allParameters\n",
    "    def minimizeCost(self,allParameters,X,y,regFactor,optMethod = None, maxIterations = 250):\n",
    "        self.fmin = optimize.minimize(fun=self.getCost,x0=allParameters,method = optMethod, \n",
    "                                      jac = self.backPropagate ,args=(X,y,regFactor), options = {'maxiter':maxIterations})\n",
    "        return self.fmin\n",
    "    def predict(self,allParameters,X,y):\n",
    "        self.unrollThetas(allParameters)\n",
    "        self.forwardPropagate(X,self.theta1,self.theta2)\n",
    "        '''\n",
    "        Remember that labels go from 1 - 10.\n",
    "        '''\n",
    "        self.allPredictions = np.argmax(self.hypothesis, axis =1) + 1\n",
    "        self.predictionAccuracy = round(100*np.mean(self.allPredictions == y),2)\n",
    "        return self.predictionAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1 = weights['Theta1'] #25x401\n",
    "theta2= weights['Theta2'] #10x26\n",
    "'''\n",
    "Roll all the weights into a 1-D array.  This is necessary because the optmization algorithms \n",
    "only accept one parameter for the weights.\n",
    "'''\n",
    "nn = NeuralNetwork(inputSize+1,hiddenSize+1,outputSize)\n",
    "initialTheta = nn.rollThetas(theta1,theta2)\n",
    "initialTheta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost with lambda = 0: 0.287629165161319\n",
      "Cost with lambda = 1: 0.3837698590909237\n",
      "\n",
      "Sigmoid Gradient evaluated at [[-1.  -0.5  0.   0.5  1. ]]:\n",
      "[[0.19661193 0.23500371 0.25       0.23500371 0.19661193]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check that the unregularized and regularized portions of cost function are working properly.\n",
    "'''\n",
    "for regFactor in [0,1]:\n",
    "    cost = nn.getCost(initialTheta,X,y,regFactor)\n",
    "    print('Cost with lambda = ' + str(regFactor) + ': ' + str(cost[0,0]))\n",
    "'''\n",
    "Check that the gradient of the sigmoid function is being calculated correctly.\n",
    "'''\n",
    "dummy = np.matrix([-1, -0.5, 0, 0.5, 1])\n",
    "g = nn.sigmoidGrad(dummy)\n",
    "print('\\nSigmoid Gradient evaluated at ' + str(dummy) + ':')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1383 | numericalGrad: 1.517e-04 | backPropagate: 1.517e-04\n",
      "Index: 1024 | numericalGrad: 3.543e-04 | backPropagate: 3.543e-04\n",
      "Index: 8260 | numericalGrad: -1.438e-05 | backPropagate: -1.438e-05\n",
      "Index: 62 | numericalGrad: -1.757e-05 | backPropagate: -1.757e-05\n",
      "Index: 10064 | numericalGrad: 1.868e-01 | backPropagate: 1.868e-01\n",
      "Index: 5985 | numericalGrad: -1.281e-05 | backPropagate: -1.281e-05\n",
      "Index: 4257 | numericalGrad: 1.048e-02 | backPropagate: 1.048e-02\n",
      "Index: 3664 | numericalGrad: 1.549e-04 | backPropagate: 1.549e-04\n",
      "Index: 10260 | numericalGrad: 1.865e-01 | backPropagate: 1.865e-01\n",
      "Index: 6798 | numericalGrad: -2.116e-05 | backPropagate: -2.116e-05\n",
      "\n",
      "The relative difference between the two solutions is 5.028e-11; this value should be less than 1e-9 if backPropagate was implemented correctly.\n",
      "\n",
      "Note that we printed 10 of the 25 gradient comparison(s) that were made.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Randomly initialize weights.  Avoid initializing weights to zeros because that will lead to identical parameters going\n",
    "into each hidden unit for each input.\n",
    "'''\n",
    "randWeights = nn.randInitializeWeights()\n",
    "numComparisons = 25\n",
    "'''\n",
    "Randomly compare numComparisons of the gradient values from the backpropagation algorithm to those calculated numerically and \n",
    "ensure that they are close in value.  Avoid using the numericalGrad algorithm in when minimizing the cost function because it \n",
    "is slow.\n",
    "'''\n",
    "normDiff, numGrad,bpGrad,randIndexes = nn.checkNNGradient(randWeights,X,y,regFactor,numComparisons)\n",
    "numLoops = numComparisons\n",
    "numMaxLoops = 10\n",
    "'''\n",
    "Let's avoid printing out more than numMaxLoops comparisons.\n",
    "'''\n",
    "try:\n",
    "    for i in range(0,numMaxLoops):\n",
    "        numGradient = '{:.3e}'.format(float(numGrad[i]))\n",
    "        bpGradient = '{:.3e}'.format(float(bpGrad[i]))\n",
    "        print('Index: %s | numericalGrad: %s | backPropagate: %s' %(randIndexes[i],numGradient,bpGradient))\n",
    "    numLoops = numMaxLoops\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "normDiff = '{:.3e}'.format(float(normDiff))\n",
    "print('\\nThe relative difference between the two solutions is %s; this value should be ' \\\n",
    "      'less than 1e-9 if backPropagate was implemented correctly.' %(normDiff))\n",
    "print('\\nNote that we printed %s of the %s gradient comparison(s) that were made.' %(numLoops,numComparisons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: matrix([[0.33534688]])\n",
      "     jac: array([ 3.91553012e-05, -1.11518225e-06, -6.70050235e-07, ...,\n",
      "       -3.39656583e-04, -1.88747701e-04, -1.02241037e-04])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 250\n",
      "     nit: 19\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([-0.7217267 , -0.00557591, -0.00335025, ...,  0.87218384,\n",
      "       -0.09506831,  2.52653745])\n",
      "\n",
      "Prediction accuracy: 99.16%.\n",
      "Note that we used the TNC optimization algorithm to minimize the cost function.\n"
     ]
    }
   ],
   "source": [
    "regFactor = 1\n",
    "optMethod = 'TNC' #Truncated Newton Method.\n",
    "maxIterations = 250 \n",
    "'''\n",
    "Minimize the cost function.\n",
    "'''\n",
    "fmin = nn.minimizeCost(randWeights,X,y,regFactor,optMethod,maxIterations)\n",
    "print(fmin)\n",
    "calcWeights = fmin['x']\n",
    "'''\n",
    "Use the weights that minimized the cost function to predict values.\n",
    "'''\n",
    "nn.predict(calcWeights,X,y)\n",
    "\n",
    "print('\\nPrediction accuracy: %s%%.' %(nn.predictionAccuracy))\n",
    "print('Note that we used the %s optimization algorithm to minimize the cost function.' %(optMethod))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
